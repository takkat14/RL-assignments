{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ru_4OlKkRJs"
      },
      "source": [
        "# Implementing Advantage-Actor Critic (A2C) - 4 pts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSDb2Y9XkRJu"
      },
      "source": [
        "In this notebook you will implement Advantage Actor Critic algorithm that trains on a batch of Atari 2600 environments running in parallel. \n",
        "\n",
        "Firstly, we will use environment wrappers implemented in file `atari_wrappers.py`. These wrappers preprocess observations (resize, grayscal, take max between frames, skip frames, stack them together, prepares for PyTorch and normalizes to [0, 1]) and rewards. Some of the wrappers help to reset the environment and pass `done` flag equal to `True` when agent dies.\n",
        "File `env_batch.py` includes implementation of `ParallelEnvBatch` class that allows to run multiple environments in parallel. To create an environment we can use `nature_dqn_env` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EzOj1ZlisBkg"
      },
      "outputs": [],
      "source": [
        "# !pip3 install gym==0.25.2\n",
        "# !pip3 install -U gym[atari,accept-rom-license]==0.25.2\n",
        "# !pip3 install tensorboardX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%load_ext tensorboard\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/paperspace/RL-assignments/hw4\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:43:34.476855Z",
          "start_time": "2020-09-11T09:43:32.696812Z"
        },
        "id": "6RsSY8ejkRJv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/paperspace/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "\n",
            "A.L.E: Arcade Learning Environment (version 0.7.5+db37282)\n",
            "[Powered by Stella]\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "/home/paperspace/.local/lib/python3.8/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "%autoreload 2\n",
        "from atari_wrappers import nature_dqn_env\n",
        "\n",
        "nenvs = 8    # change this if you have more than 8 CPU ;)\n",
        "\n",
        "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=nenvs)\n",
        "\n",
        "n_actions = env.action_space.spaces[0].n\n",
        "obs = env.reset()\n",
        "assert obs.shape == (nenvs, 4, 84, 84)\n",
        "assert obs.dtype == np.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/home/paperspace/RL-assignments/hw4\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwGObAsekRJv"
      },
      "source": [
        "Next, we will need to implement a model that predicts logits of policy distribution and critic value. Use shared backbone. You may use same architecture as in DQN task with one modification: instead of having a single output layer, it must have two output layers taking as input the output of the last hidden layer (one for actor, one for critic). \n",
        "\n",
        "Still it may be very helpful to make more changes:\n",
        "* use orthogonal initialization with gain $\\sqrt{2}$ and initialize biases with zeros;\n",
        "* use more filters (e.g. 32-64-64 instead of 16-32-64);\n",
        "* use two-layer heads for actor and critic or add a linear layer into backbone;\n",
        "\n",
        "**Danger:** do not divide on 255, input is already normalized to [0, 1] in our wrappers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "seed = 0xDEAD\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        nn.init.orthogonal_(m.weight.data, math.sqrt(2.0))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:15.493948Z",
          "start_time": "2020-09-11T05:45:15.065561Z"
        },
        "id": "6nhH1MopkRJv"
      },
      "outputs": [],
      "source": [
        "class A2CNetwork(nn.Module):\n",
        "    def __init__(self, n_actions, state_shape, device):\n",
        "            \n",
        "            super().__init__()\n",
        "\n",
        "            self.n_actions = n_actions\n",
        "            self.state_shape = state_shape\n",
        "            self.device = device\n",
        "            \n",
        "            b, c, h, w = state_shape\n",
        "\n",
        "            self.backbone = nn.Sequential(\n",
        "                nn.Conv2d(c, 32, kernel_size=8, stride=4),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "                nn.ReLU(),\n",
        "                nn.Flatten(),\n",
        "            ).apply(init_weights).to(self.device)\n",
        "\n",
        "            \n",
        "\n",
        "        \n",
        "            self.fc = nn.Sequential(\n",
        "                nn.Linear(3136, 512),\n",
        "                nn.ReLU()\n",
        "            ).apply(init_weights).to(self.device)\n",
        "\n",
        "            self.V =nn.Sequential(\n",
        "                    nn.Linear(512, 512 // 4),\n",
        "                    nn.LeakyReLU(0.2),\n",
        "                    nn.Linear(512 // 4, 1)\n",
        "        ).apply(init_weights).to(self.device)\n",
        "            self.logits = nn.Linear(512, n_actions).apply(init_weights).to(self.device)\n",
        "\n",
        "    def forward(self, state_t):\n",
        "            \"\"\"\n",
        "            Takes agent's previous step and observation, \n",
        "            returns next state and whatever it needs to learn (tf tensors)\n",
        "            \"\"\"\n",
        "            state_t = torch.tensor(state_t, device=self.device, dtype=torch.float);\n",
        "\n",
        "            state_t = self.fc(self.backbone(state_t))\n",
        "        \n",
        "            logits = self.logits(state_t)\n",
        "            state_value = self.V(state_t)[:, 0]\n",
        "            return logits, state_value\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EpyEfjHkRJw"
      },
      "source": [
        "You will also need to define and use a policy that wraps the model. While the model computes logits for all actions, the policy will sample actions and also compute their log probabilities.  `policy.act` should return a **dictionary** of all the arrays that are needed to interact with an environment and train the model.\n",
        "\n",
        "**Important**: \"actions\" will be sent to environment, they must be numpy array or list, not PyTorch tensor.\n",
        "\n",
        "Note: you can add more keys, e.g. it can be convenient to compute entropy right here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:15.508916Z",
          "start_time": "2020-09-11T05:45:15.494964Z"
        },
        "id": "ey5W80QckRJw"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Categorical\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Policy:\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "\n",
        "    def sample_actions(self, logits):\n",
        "        \"\"\"pick actions given numeric agent outputs (np arrays)\"\"\"\n",
        "        policy =  F.softmax(logits, -1).detach().cpu()\n",
        "        actions = torch.multinomial(policy, num_samples=1).detach().numpy().flatten()\n",
        "        return actions\n",
        "\n",
        "    def log_probs(self, logits, actions):\n",
        "         return nn.functional.log_softmax(logits, -1)[np.arange(len(actions)), actions]\n",
        "\n",
        "\n",
        "    def act(self, inputs):\n",
        "        '''\n",
        "        input:\n",
        "            inputs - numpy array, (batch_size x channels x width x height)\n",
        "        output: dict containing keys ['actions', 'logits', 'log_probs', 'values']:\n",
        "            'actions' - selected actions, numpy, (batch_size)\n",
        "            'logits' - actions logits, tensor, (batch_size x num_actions)\n",
        "            'log_probs' - log probs of selected actions, tensor, (batch_size)\n",
        "            'values' - critic estimations, tensor, (batch_size)\n",
        "        '''\n",
        "        agent_outputs = self.model(inputs)\n",
        "        logits, state_values = agent_outputs\n",
        "        actions = self.sample_actions(logits)\n",
        "        log_probs = self.log_probs(logits, actions) \n",
        "        \n",
        "        return {\n",
        "            \"actions\": actions,\n",
        "            \"logits\": logits,\n",
        "            \"log_probs\": log_probs,\n",
        "            \"values\": state_values,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4YaKtT1kRJw"
      },
      "source": [
        "Next we will pass the environment and policy to a runner that collects rollouts from the environment. \n",
        "The class is already implemented for you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:15.635649Z",
          "start_time": "2020-09-11T05:45:15.510915Z"
        },
        "id": "I3Czcvt7kRJx"
      },
      "outputs": [],
      "source": [
        "from runners import EnvRunner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UoZPBCHkRJx"
      },
      "source": [
        "This runner interacts with the environment for a given number of steps and returns a dictionary containing\n",
        "keys \n",
        "\n",
        "* 'observations' \n",
        "* 'rewards' \n",
        "* 'dones'\n",
        "* 'actions'\n",
        "* all other keys that you defined in `Policy`\n",
        "\n",
        "under each of these keys there is a python `list` of interactions with the environment of specified length $T$ &mdash; the size of partial trajectory, or rollout length. Let's have a look at how it works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:17.267442Z",
          "start_time": "2020-09-11T05:45:15.636676Z"
        },
        "id": "WvP1M-8gkRJx"
      },
      "outputs": [],
      "source": [
        "model = A2CNetwork(n_actions, obs.shape, device)\n",
        "policy = Policy(model)\n",
        "runner = EnvRunner(env, policy, nsteps=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.244263Z",
          "start_time": "2020-09-11T05:45:17.268410Z"
        },
        "id": "rgek3SOJkRJx"
      },
      "outputs": [],
      "source": [
        "# generates new rollout\n",
        "trajectory = runner.get_next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.260254Z",
          "start_time": "2020-09-11T05:45:18.245263Z"
        },
        "id": "X8I8NqJYkRJx"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dict_keys(['actions', 'logits', 'log_probs', 'values', 'observations', 'rewards', 'dones'])\n"
          ]
        }
      ],
      "source": [
        "# what is inside\n",
        "print(trajectory.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.418163Z",
          "start_time": "2020-09-11T05:45:18.262253Z"
        },
        "id": "Cleh8T7_kRJx"
      },
      "outputs": [],
      "source": [
        "# Sanity checks\n",
        "assert 'logits' in trajectory, \"Not found: policy didn't provide logits\"\n",
        "assert 'log_probs' in trajectory, \"Not found: policy didn't provide log_probs of selected actions\"\n",
        "assert 'values' in trajectory, \"Not found: policy didn't provide critic estimations\"\n",
        "assert trajectory['logits'][0].shape == (nenvs, n_actions), \"logits wrong shape\"\n",
        "assert trajectory['log_probs'][0].shape == (nenvs,), \"log_probs wrong shape\"\n",
        "assert trajectory['values'][0].shape == (nenvs,), \"values wrong shape\"\n",
        "\n",
        "for key in trajectory.keys():\n",
        "    assert len(trajectory[key]) == 5, \\\n",
        "    f\"something went wrong: 5 steps should have been done, got trajectory of length {len(trajectory[key])} for '{key}'\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDKUeUlfkRJx"
      },
      "source": [
        "Now let's work with this trajectory a bit. To train the critic you will need to compute the value targets. It will also be used as an estimation of $Q$ for actor training.\n",
        "\n",
        "You should use all available rewards for value targets, so the formula for the value targets is simple:\n",
        "\n",
        "$$\n",
        "\\hat v(s_t) = \\sum_{t'=0}^{T - 1}\\gamma^{t'}r_{t+t'} + \\gamma^T \\hat{v}(s_{t+T}),\n",
        "$$\n",
        "\n",
        "where $s_{t + T}$ is the latest observation of the environment.\n",
        "\n",
        "Any callable could be passed to `EnvRunner` to be applied to each partial trajectory after it is collected. \n",
        "Thus, we can implement and use `ComputeValueTargets` callable. \n",
        "\n",
        "**Do not forget** to use `trajectory['dones']` flags to check if you need to add the value targets at the next step when \n",
        "computing value targets for the current step.\n",
        "\n",
        "**Bonus (+1 pt):** implement [Generalized Advantage Estimation (GAE)](https://arxiv.org/pdf/1506.02438.pdf) instead; use $\\lambda \\approx 0.95$ or even closer to 1 in experiment. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T10:29:23.189515Z",
          "start_time": "2020-09-11T10:29:23.173510Z"
        },
        "id": "RshQcHgPkRJy"
      },
      "outputs": [],
      "source": [
        "class ComputeValueTargets:\n",
        "    def __init__(self, policy, gamma=0.99, lambda_=0.95):\n",
        "        self.policy = policy\n",
        "        self.gamma = gamma\n",
        "        self.lambda_ = lambda_\n",
        "        \n",
        "    def __call__(self, trajectory, latest_observation):\n",
        "        '''\n",
        "        This method should modify trajectory inplace by adding \n",
        "        an item with key 'value_targets' to it\n",
        "        \n",
        "        input:\n",
        "            trajectory - dict from runner\n",
        "            latest_observation - last state, numpy, (num_envs x channels x width x height)\n",
        "        '''\n",
        "        value_target = self.policy.act(latest_observation)['values'][-1]\n",
        "        env_steps = len(trajectory['dones'])\n",
        "        rewards = torch.tensor(trajectory['rewards'], device=value_target.device)\n",
        "        dones = torch.tensor(trajectory['dones'], device=value_target.device, dtype=torch.float32)\n",
        "        is_not_done = 1 - dones\n",
        "        trajectory['advantages'] = [None for _ in range(env_steps)]\n",
        "        trajectory['value_targets'] = [None for _ in range(env_steps)]\n",
        "        gae = 0\n",
        "        for step in range(env_steps - 1, -1, -1):\n",
        "            delta = rewards[step] - trajectory['values'][step]\n",
        "            if step == env_steps - 1:\n",
        "                delta += self.gamma * value_target * is_not_done[step]\n",
        "            else:\n",
        "                delta += self.gamma * trajectory['values'][step + 1] * is_not_done[step]\n",
        "\n",
        "            gae = delta + self.gamma * self.lambda_ * is_not_done[step] * gae\n",
        "            trajectory['advantages'][step] = gae\n",
        "            trajectory['value_targets'][step] = gae + trajectory['values'][step]\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rDrCV9XEkRJy"
      },
      "source": [
        "After computing value targets we will transform lists of interactions into tensors\n",
        "with the first dimension `batch_size` which is equal to `T * nenvs`.\n",
        "\n",
        "You need to make sure that after this transformation `\"log_probs\"`, `\"value_targets\"`, `\"values\"` are 1-dimensional PyTorch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.623944Z",
          "start_time": "2020-09-11T05:45:18.515153Z"
        },
        "id": "noKYEi4ikRJy"
      },
      "outputs": [],
      "source": [
        "class MergeTimeBatch:\n",
        "    \"\"\" Merges first two axes typically representing time and env batch. \"\"\"\n",
        "    def __call__(self, trajectory, latest_observation):\n",
        "        for key, val in trajectory.items():\n",
        "            if key in ['actions', 'observations', 'rewards', 'dones']:\n",
        "                val = torch.tensor(val)\n",
        "            elif key in ['logits', 'log_probs', 'values', 'value_targets', 'advantages']:\n",
        "                val = torch.stack(val)\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            val = val.reshape(-1, *val.shape[2:])\n",
        "            trajectory[key] = val"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXirNkSDkRJy"
      },
      "source": [
        "Let's do more sanity checks!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.782710Z",
          "start_time": "2020-09-11T05:45:18.624949Z"
        },
        "id": "zSATsAIlkRJy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_9375/866117795.py:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  rewards = torch.tensor(trajectory['rewards'], device=value_target.device)\n",
            "/tmp/ipykernel_9375/1472564247.py:6: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  val = torch.tensor(val)\n"
          ]
        }
      ],
      "source": [
        "runner = EnvRunner(env, policy, nsteps=5, transforms=[ComputeValueTargets(policy),\n",
        "                                                      MergeTimeBatch()])\n",
        "\n",
        "trajectory = runner.get_next()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.878653Z",
          "start_time": "2020-09-11T05:45:18.784709Z"
        },
        "id": "q7F9O4_HkRJy"
      },
      "outputs": [],
      "source": [
        "# More sanity checks\n",
        "assert 'value_targets' in trajectory, \"Value targets not found\"\n",
        "assert trajectory['log_probs'].shape == (5 * nenvs,)\n",
        "assert trajectory['value_targets'].shape == (5 * nenvs,)\n",
        "assert trajectory['values'].shape == (5 * nenvs,)\n",
        "\n",
        "assert trajectory['log_probs'].requires_grad, \"Gradients are not available for actor head!\"\n",
        "assert trajectory['values'].requires_grad, \"Gradients are not available for critic head!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMTYNSe_kRJy"
      },
      "source": [
        "Now is the time to implement the advantage actor critic algorithm itself. You can look into [Mnih et al. 2016](https://arxiv.org/abs/1602.01783) paper, and lectures ([part 1](https://www.youtube.com/watch?v=Ds1trXd6pos&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=5), [part 2](https://www.youtube.com/watch?v=EKqxumCuAAY&list=PLkFD6_40KJIwhWJpGazJ9VSj9CFMkb79A&index=6)) by Sergey Levine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Actor-critic objective\n",
        "\n",
        "Here we define a loss function that uses rollout above to train advantage actor-critic agent.\n",
        "\n",
        "\n",
        "Our loss consists of three components:\n",
        "\n",
        "* __The policy \"loss\"__\n",
        " $$ \\hat{J}_{policy} = {1 \\over T} \\cdot \\sum_t { \\log \\pi(a_t | s_t) } \\cdot A_{const}(s,a) $$\n",
        "  * This function has no meaning in and of itself, but it was built such that\n",
        "  * $ \\nabla \\hat J_{policy} = {1 \\over N} \\cdot \\sum_t { \\nabla \\log \\pi(a_t | s_t) } \\cdot A(s,a) \\approx \\nabla E_{s, a \\sim \\pi} R(s,a) $\n",
        "  * Therefore if we __maximize__ $\\hat{J}_{policy}$ with gradient ascent we will maximize expected reward\n",
        "  \n",
        "  \n",
        "* __The value \"loss\"__\n",
        "  $$ L_{value} = {1 \\over T} \\cdot \\sum_t { [r + \\gamma \\cdot V_{const}(s_{t+1}) - V(s_t)] ^ 2 }$$\n",
        "  * Old good TD-loss from q-learning and alike\n",
        "  * If we minimize this loss, $V(s)$ will converge to $V_\\pi(s) = E_{a \\sim \\pi(a | s)} R(s,a) $\n",
        "  * Note that target can be N-step in the same way as $A(s,a)$\n",
        "\n",
        "\n",
        "* __Entropy Regularizer__\n",
        "  $$ H = - {1 \\over T} \\sum_t \\sum_a {\\pi(a|s_t) \\cdot \\log \\pi (a|s_t)}$$\n",
        "  * If we __maximize__ entropy we discourage agent from predicting zero probability to actions\n",
        "  prematurely (a.k.a. exploration)\n",
        "  \n",
        "  \n",
        "So we optimize a linear combination of $L_{value} - \\hat J_{policy} -H$\n",
        "  \n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "__One more thing:__ since we train on T-step rollouts, we can use N-step formula for advantage for free:\n",
        "  * At the last step, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot V(s_{t+1}) - V(s_t) $\n",
        "  * One step earlier, $A(s_t,a_t) = r(s_t, a_t) + \\gamma \\cdot r(s_{t+1}, a_{t+1}) + \\gamma ^ 2 \\cdot V(s_{t+2}) - V(s_t) $\n",
        "  * Et cetera, et cetera. This way agent starts training much faster since it's estimate of A(s,a) depends less on his (imperfect) value function and more on actual rewards. There's also a [nice generalization](https://arxiv.org/abs/1506.02438) of this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['actions', 'logits', 'log_probs', 'values', 'observations', 'rewards', 'dones', 'advantages', 'value_targets'])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trajectory.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:18.990212Z",
          "start_time": "2020-09-11T05:45:18.880653Z"
        },
        "id": "X4Sn1j8skRJy"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "class A2C:\n",
        "    def __init__(self, policy, optimizer, value_loss_coef=0.25, entropy_coef=0.01, max_grad_norm=0.5):\n",
        "        self.policy = policy\n",
        "        self.optimizer = optimizer\n",
        "        self.value_loss_coef = value_loss_coef\n",
        "        self.entropy_coef = entropy_coef\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "    def loss(self, trajectory, write):\n",
        "        # compute all losses\n",
        "        # do not forget to use weights for critic loss and entropy loss\n",
        "        \n",
        "        advantages = trajectory['value_targets'] - trajectory['values']\n",
        "        policy_loss =  -torch.mean(trajectory['log_probs'] * advantages.detach())\n",
        "\n",
        "        value_loss =  (trajectory['value_targets'].detach() - trajectory['values']).pow(2).mean()   \n",
        "\n",
        "\n",
        "        entropy_loss  = -( F.log_softmax(trajectory['logits'], dim=-1) *  F.softmax(trajectory['logits'], dim=-1)).sum(-1).sum(-1).mean()\n",
        "\n",
        "        \n",
        "        # log all losses\n",
        "        write('losses', {\n",
        "            'policy loss': policy_loss,\n",
        "            'critic loss': value_loss,\n",
        "            'entropy loss': entropy_loss,\n",
        "        })\n",
        "        \n",
        "        # additional logs\n",
        "        write('critic/advantage', advantages.mean())\n",
        "        write('critic/values', {\n",
        "            'value predictions':  trajectory['values'].mean(),\n",
        "            'value targets': trajectory['value_targets'].detach().mean(),\n",
        "        })\n",
        "       \n",
        "        \n",
        "        # return scalar loss\n",
        "        return self.value_loss_coef * value_loss + policy_loss - self.entropy_coef * entropy_loss            \n",
        "\n",
        "    def train(self, runner):\n",
        "        # collect trajectory using runner\n",
        "        # compute loss and perform one step of gradient optimization\n",
        "        # do not forget to clip gradients\n",
        "        self.policy.model.train()\n",
        "        trajectory = runner.get_next()\n",
        "        loss = self.loss(trajectory, runner.write)\n",
        "        loss.backward()\n",
        "        grad_norm = nn.utils.clip_grad_norm_(self.policy.model.parameters(), self.max_grad_norm)\n",
        "        self.optimizer.step()\n",
        "        self.optimizer.zero_grad()\n",
        "        \n",
        "        # use runner.write to log scalar to tensorboard\n",
        "        runner.write('gradient norm', grad_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLEW45ixkRJz"
      },
      "source": [
        "Now you can train your model. For optimization we suggest you use RMSProp with learning rate 7e-4 (you can also linearly decay it to 0), smoothing constant (alpha in PyTorch) equal to 0.99 and epsilon equal to 1e-5.\n",
        "\n",
        "We recommend to train for at least 10 million environment steps across all batched environments (takes ~3 hours on a single GTX1080 with 8 CPU). It should be possible to achieve *average raw reward over last 100 episodes* (the average is taken over 100 last episodes in each environment in the batch) of about 600. **Your goal is to reach 500**.\n",
        "\n",
        "Notes:\n",
        "* if your reward is stuck at ~200 for more than 2M steps then probably there is a bug\n",
        "* if your gradient norm is >10 something probably went wrong\n",
        "* make sure your `entropy loss` is negative, your `critic loss` is positive\n",
        "* make sure you didn't forget `.detach` in losses where it's needed\n",
        "* `actor loss` should oscillate around zero or near it; do not expect loss to decrease in RL ;)\n",
        "* you can experiment with `nsteps` (\"rollout length\"); standard rollout length is 5 or 10. Note that this parameter influences how many algorithm iterations is required to train on 10M steps (or 40M frames --- we used frameskip in preprocessing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T05:45:19.196652Z",
          "start_time": "2020-09-11T05:45:18.991227Z"
        },
        "id": "p2uo3WfQkRJz"
      },
      "outputs": [],
      "source": [
        "model = A2CNetwork(n_actions,obs.shape, device=device)\n",
        "policy = Policy(model)\n",
        "runner = EnvRunner(env, policy, nsteps=10, transforms=[ComputeValueTargets(policy),\n",
        "                                                      MergeTimeBatch()])\n",
        "\n",
        "optimizer = torch.optim.RMSprop(model.parameters(), lr=7e-4, alpha=0.99, eps=1e-5)\n",
        "\n",
        "a2c = A2C(policy, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tqdm import trange"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:19.815094Z",
          "start_time": "2020-09-11T05:45:19.278605Z"
        },
        "id": "V-gVjNvAkRJz"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|                                                                              | 0/100000 [00:00<?, ?it/s]/tmp/ipykernel_9375/1472564247.py:6: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
            "  val = torch.tensor(val)\n",
            "  1%|â–‰                                                               | 1437/100000 [09:46<11:32:14,  2.37it/s]"
          ]
        }
      ],
      "source": [
        "for step in trange(100000): \n",
        "    a2c.train(runner)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:19.847076Z",
          "start_time": "2020-09-11T09:03:19.816094Z"
        },
        "id": "l7s42BFokRJz"
      },
      "outputs": [],
      "source": [
        "# save your model just in case \n",
        "torch.save(model.state_dict(), \"A2C\")    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:20.070164Z",
          "start_time": "2020-09-11T09:03:19.848077Z"
        },
        "id": "KaBLt8ozkRJz"
      },
      "outputs": [
        {
          "ename": "BrokenPipeError",
          "evalue": "[Errno 32] Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[1;32m/home/paperspace/RL-assignments/hw4/hw_4_advantage_actor_critic.ipynb Cell 37\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B65.19.132.213/home/paperspace/RL-assignments/hw4/hw_4_advantage_actor_critic.ipynb#X46sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m env\u001b[39m.\u001b[39;49mclose()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py:425\u001b[0m, in \u001b[0;36mWrapper.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    424\u001b[0m     \u001b[39m\"\"\"Closes the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mclose()\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py:425\u001b[0m, in \u001b[0;36mWrapper.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclose\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    424\u001b[0m     \u001b[39m\"\"\"Closes the environment.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mclose()\n",
            "File \u001b[0;32m~/RL-assignments/hw4/env_batch.py:202\u001b[0m, in \u001b[0;36mParallelEnvBatch.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m conn \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_parent_connections:\n\u001b[0;32m--> 202\u001b[0m     conn\u001b[39m.\u001b[39;49msend((\u001b[39m\"\u001b[39;49m\u001b[39mclose\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m))\n\u001b[1;32m    203\u001b[0m \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_processes:\n\u001b[1;32m    204\u001b[0m     p\u001b[39m.\u001b[39mjoin()\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:206\u001b[0m, in \u001b[0;36m_ConnectionBase.send\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_closed()\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_writable()\n\u001b[0;32m--> 206\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_bytes(_ForkingPickler\u001b[39m.\u001b[39;49mdumps(obj))\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:411\u001b[0m, in \u001b[0;36mConnection._send_bytes\u001b[0;34m(self, buf)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send(buf)\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     \u001b[39m# Issue #20540: concatenate before sending, to avoid delays due\u001b[39;00m\n\u001b[1;32m    408\u001b[0m     \u001b[39m# to Nagle's algorithm on a TCP socket.\u001b[39;00m\n\u001b[1;32m    409\u001b[0m     \u001b[39m# Also note we want to avoid sending a 0-length buffer separately,\u001b[39;00m\n\u001b[1;32m    410\u001b[0m     \u001b[39m# to avoid \"broken pipe\" errors if the other end closed the pipe.\u001b[39;00m\n\u001b[0;32m--> 411\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send(header \u001b[39m+\u001b[39;49m buf)\n",
            "File \u001b[0;32m/usr/lib/python3.8/multiprocessing/connection.py:368\u001b[0m, in \u001b[0;36mConnection._send\u001b[0;34m(self, buf, write)\u001b[0m\n\u001b[1;32m    366\u001b[0m remaining \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(buf)\n\u001b[1;32m    367\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 368\u001b[0m     n \u001b[39m=\u001b[39m write(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle, buf)\n\u001b[1;32m    369\u001b[0m     remaining \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m n\n\u001b[1;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m remaining \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe"
          ]
        }
      ],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7mzN0BukRJz"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:20.292111Z",
          "start_time": "2020-09-11T09:03:20.071165Z"
        },
        "id": "rd4KkuRjkRJz"
      },
      "outputs": [],
      "source": [
        "env = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, \n",
        "                     clip_reward=False, summaries=False, episodic_life=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:03:20.307488Z",
          "start_time": "2020-09-11T09:03:20.293112Z"
        },
        "id": "lncShpeRkRJz"
      },
      "outputs": [],
      "source": [
        "def evaluate(env, policy, n_games=1, t_max=10000):\n",
        "    '''\n",
        "    Plays n_games and returns rewards\n",
        "    '''\n",
        "    rewards = []\n",
        "    \n",
        "    for _ in range(n_games):\n",
        "        s = env.reset()\n",
        "        \n",
        "        R = 0\n",
        "        for _ in range(t_max):\n",
        "            action = policy.act(np.array([s]))[\"actions\"][0]\n",
        "            \n",
        "            s, r, done, _ = env.step(action)\n",
        "            \n",
        "            R += r\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        rewards.append(R)\n",
        "    return np.array(rewards)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.357449Z",
          "start_time": "2020-09-11T09:25:35.258389Z"
        },
        "id": "0an5jDvEkRJz"
      },
      "outputs": [],
      "source": [
        "# evaluation will take some time!\n",
        "sessions = evaluate(env, policy, n_games=30)\n",
        "score = sessions.mean()\n",
        "print(f\"Your score: {score}\")\n",
        "\n",
        "assert score >= 500, \"Needs more training?\"\n",
        "print(\"Well done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.373428Z",
          "start_time": "2020-09-11T09:26:18.359434Z"
        },
        "id": "oEvWYpY7kRJz"
      },
      "outputs": [],
      "source": [
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUjT3uFnkRJ0"
      },
      "source": [
        "## Record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.657314Z",
          "start_time": "2020-09-11T09:26:18.375172Z"
        },
        "id": "iKsBwdwskRJ0"
      },
      "outputs": [],
      "source": [
        "env_monitor = nature_dqn_env(\"SpaceInvadersNoFrameskip-v4\", nenvs=None, monitor=True,\n",
        "                             clip_reward=False, summaries=False, episodic_life=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:38:24.509378Z",
          "start_time": "2020-09-11T09:38:16.392351Z"
        },
        "id": "GSfN0bdokRJ0",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# record sessions\n",
        "sessions = evaluate(env_monitor, policy, n_games=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:38:51.559623Z",
          "start_time": "2020-09-11T09:38:51.521317Z"
        },
        "id": "od4XjKhPkRJ0"
      },
      "outputs": [],
      "source": [
        "# rewards for recorded games\n",
        "sessions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-11T09:26:18.373428Z",
          "start_time": "2020-09-11T09:26:18.359434Z"
        },
        "id": "DkvqNvG1kRJ0"
      },
      "outputs": [],
      "source": [
        "env_monitor.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
